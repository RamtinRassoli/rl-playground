{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch as tp\n",
    "import torch.nn as nn\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "1. Play N number of episodes using our current model and environment.\n",
    "2. Calculate the total reward for every episode and decide on a reward\n",
    "boundary. Usually, we use some percentile of all rewards, such as 50th or\n",
    "70th.\n",
    "3. Throw away all episodes with a reward below the boundary.\n",
    "4. Train on the remaining \"elite\" episodes using observations as the input and\n",
    "issued actions as the desired output.\n",
    "5. Repeat from step 1 until we become satisfied with the result.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes, lr):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size) \n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)  \n",
    "        \n",
    "        self.criterion = nn.BCELoss()\n",
    "        self.optimizer = tp.optim.Adam(self.parameters(), lr=lr)  \n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return tp.sigmoid(out)\n",
    "    \n",
    "    def train(self, episodes, labels):\n",
    "        outputs = model(episodes)\n",
    "        loss = self.criterion(outputs, labels)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_discounted_rewards(rewards, gamma):\n",
    "    gammas = np.array([gamma**i for i in range(len(rewards))])\n",
    "    return np.sum(gammas * np.array(rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_for_episodes(episodes, model):\n",
    "    input_x = np.empty((0, 4))\n",
    "    labels_y = np.empty((0, 1))\n",
    "    for ep in episodes:\n",
    "        input_x = np.append(input_x, ep.observations,  axis=0)\n",
    "        labels_y = np.append(labels_y, ep.actions,  axis=0)\n",
    "    \n",
    "    model.train(Variable(tp.from_numpy(input_x)), Variable(tp.from_numpy(labels_y)))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Episode:\n",
    "    def __init__(self, action_shape, observation_shape):\n",
    "        self.total_reward = 0\n",
    "        self.actions = np.empty(action_shape)\n",
    "        self.observations = np.empty(observation_shape)\n",
    "        self.rewards = np.empty(0)\n",
    "        \n",
    "    def add_action(self, action):\n",
    "        self.actions = np.append(self.actions, [action],  axis=0)\n",
    "        \n",
    "    def add_reward(self, reward):\n",
    "        self.rewards = np.append(self.rewards, [reward],  axis=0)\n",
    "        \n",
    "    def add_observation(self, observation):\n",
    "        self.observations = np.append(self.observations, [observation],  axis=0)\n",
    "        \n",
    "    def calculate_total_reward(self):\n",
    "        self.total_reward = get_discounted_rewards(self.rewards, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "action space:  Discrete(2)\n",
      "state space:  Box(4,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ramtinr/Desktop/Udacity-DRLND/code/gym/gym/__init__.py:22: UserWarning: DEPRECATION WARNING: to improve load times, gym no longer automatically loads gym.spaces. Please run \"import gym.spaces\" to load gym.spaces on your own. This warning will turn into an error in a future version of gym.\n",
      "  warnings.warn('DEPRECATION WARNING: to improve load times, gym no longer automatically loads gym.spaces. Please run \"import gym.spaces\" to load gym.spaces on your own. This warning will turn into an error in a future version of gym.')\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v0\")\n",
    "observation = env.reset()\n",
    "print(\"action space: \", env.action_space)\n",
    "print(\"state space: \", env.observation_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_elite_episodes(episodes, percentile=75):\n",
    "    episodes.sort(key=lambda x: x.total_reward, reverse=True)\n",
    "    rewards = [ep.total_reward for ep in episodes]\n",
    "    pt = np.percentile(rewards, percentile)\n",
    "    \n",
    "    return [ep for ep in episodes if ep.total_reward >= pt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = tp.device('cuda' if tp.cuda.is_available() else 'cpu')\n",
    "model = NeuralNet(4, 32, 1, 0.15).to(device)\n",
    "model = model.double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ramtinr/Desktop/Udacity-DRLND/code/gym/gym/__init__.py:22: UserWarning: DEPRECATION WARNING: to improve load times, gym no longer automatically loads gym.spaces. Please run \"import gym.spaces\" to load gym.spaces on your own. This warning will turn into an error in a future version of gym.\n",
      "  warnings.warn('DEPRECATION WARNING: to improve load times, gym no longer automatically loads gym.spaces. Please run \"import gym.spaces\" to load gym.spaces on your own. This warning will turn into an error in a future version of gym.')\n",
      "/Users/ramtinr/anaconda3/envs/drlnd/lib/python3.6/site-packages/numpy/core/fromnumeric.py:2920: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/Users/ramtinr/anaconda3/envs/drlnd/lib/python3.6/site-packages/numpy/core/_methods.py:85: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "Episode: 100, total reward: 10.0\n",
      "Average Score:  20.03\n",
      "Episode: 200, total reward: 11.0\n",
      "Average Score:  27.58\n",
      "Episode: 300, total reward: 33.0\n",
      "Average Score:  51.87\n",
      "Episode: 400, total reward: 89.0\n",
      "Average Score:  62.94\n",
      "Episode: 500, total reward: 69.0\n",
      "Average Score:  110.11\n",
      "Episode: 600, total reward: 142.0\n",
      "Average Score:  124.5\n",
      "Episode: 700, total reward: 153.0\n",
      "Average Score:  136.42\n",
      "Episode: 800, total reward: 200.0\n",
      "Average Score:  175.45\n",
      "Episode: 900, total reward: 200.0\n",
      "Average Score:  184.76\n",
      "Problem solved\n"
     ]
    }
   ],
   "source": [
    "from torch.autograd import Variable\n",
    "env = gym.make(\"CartPole-v0\")\n",
    "scores = deque([], maxlen=100)\n",
    "counter = 0\n",
    "gamma = 1\n",
    "\n",
    "while True:\n",
    "    counter += 1\n",
    "    if np.mean(scores) >= 195:\n",
    "        break\n",
    "        \n",
    "    observation = env.reset()\n",
    "    episode = Episode((0, 1), (0, 4))\n",
    "    \n",
    "    for _ in range(1000):\n",
    "      probs = model.forward(Variable(tp.from_numpy(observation)))\n",
    "      p = probs.detach().numpy()[0]\n",
    "      action = np.random.choice(2, 1, p=[1-p, p]) \n",
    "      episode.add_observation(observation)\n",
    "      episode.add_action(action)\n",
    "      observation, reward, done, info = env.step(action[0])\n",
    "      episode.add_reward(reward)\n",
    "      \n",
    "      if done:\n",
    "        episode.calculate_total_reward()\n",
    "        episodes.append(episode)\n",
    "        scores.append(episode.total_reward)\n",
    "        if np.mean(scores) >= 195:\n",
    "            print(\"Problem solved\")\n",
    "        \n",
    "        if counter % 100 == 0:\n",
    "            print(\"Episode: {}, total reward: {}\".format(counter, episode.total_reward))\n",
    "        break\n",
    "    \n",
    "    if counter % 100 == 0:\n",
    "        print(\"Average Score: \", np.mean(scores))\n",
    "        episodes = get_elite_episodes(episodes, percentile=75)\n",
    "        model = train_for_episodes(episodes, model)\n",
    "        episodes = []\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
